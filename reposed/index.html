<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <meta name="title" content="RePoseD: Efficient Relative Pose Estimation With Known Depth Information (ICCV 2025 Oral)">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="Project page for RePoseD: Efficient methods for relative pose estimation using depths estimated by monocular depth estimation networks.">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="3D Vision, Pose Estimation, SLAM, Monocular Depth Estimation">
  <!-- TODO: List all authors -->
  <meta name="author" content="Yaqing Ding, Viktor Kocur, Václav Vávra, Zuzana Berger Haladová, Jian Yang, Torsten Sattler, Zuzana Kukelova">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">  
  <meta property="og:site_name" content="VRG CTU, Comenius University Bratislava, Nankai University, CIIRC CTU">  
  <meta property="og:title" content="RePoseD: Efficient Relative Pose Estimation With Known Depth Information (ICCV 2025 Oral)">  
  <meta property="og:description" content="We provide several novel solvers for relative pose with known or estimated depth and show that using them leads to better results than other geometry-based or learning-based methods. We also show how our method is able to improve depth estimates by also providing the relative scale and shifts of depth maps.">  
  <meta property="og:url" content="https://kocurvik.github.io/reposed">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://kocurvik.github.io/reposed/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="RePoseD - Research Preview">
  <meta property="article:published_time" content="2024-01-01T00:00:00.000Z">
  <meta property="article:author" content="Yaqing Ding, Viktor Kocur, Václav Vávra, Zuzana Berger Haladová, Jian Yang, Torsten Sattler, Zuzana Kukelova">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="3D vision">
  <meta property="article:tag" content="relative pose">  

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@kocurvik">
  <meta name="twitter:creator" content="@kocurvik">
  <meta name="twitter:title" content="RePoseD: Efficient Relative Pose Estimation With Known Depth Information (ICCV 2025 Oral)">
  <meta name="twitter:description" content="We provide several novel solvers for relative pose with known or estimated depth and show that using them leads to better results than other geometry-based or learning-based methods. We also show how our method is able to improve depth estimates by also providing the relative scale and shifts of depth maps.">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image" content="https://kocurvik.github.io/reposed/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="RePoseD - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="RePoseD: Efficient Relative Pose Estimation With Known Depth Information">
  <meta name="citation_author" content="Yaqing, Ding">
  <meta name="citation_author" content="Viktor, Kocur">
  <meta name="citation_author" content="Václav, Vávra">
  <meta name="citation_author" content="Zuzana, Berger Haladová">
  <meta name="citation_author" content="Jian, Yang">
  <meta name="citation_author" content="Torsten, Sattler">
  <meta name="citation_author" content="Zuzana, Kukelova">
  <meta name="citation_publication_date" content="2025">
  <meta name="citation_conference_title" content="ICCV">
  <meta name="citation_pdf_url" content="https://arxiv.org/pdf/2501.07742">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>RePoseD: Efficient Relative Pose Estimation With Known Depth Information (ICCV 2025 Oral)</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/vrg.png">
  <link rel="apple-touch-icon" href="static/images/vrg.png">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- typeset latex -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    // MathJax configuration to use $...$ for inline math and $$...$$ for display math
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      }
    };
  </script>
  
  <style>
  /* Custom CSS to center tables */
  .centered-table {
    
    width: auto; 
    margin-left: auto;
    margin-right: auto;
    /* Optional: Remove the 100% width default from Bulma for the is-fullwidth class */
    /* If you keep is-fullwidth on a table, it will stretch to 100% and this centering won't work */
  }
</style>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "RePoseD: Efficient Relative Pose Estimation With Known Depth Information",
    "description": "We provide several novel solvers for relative pose with known or estimated depth and show that using them leads to better results than other geometry-based or learning-based methods. We also show how our method is able to improve depth estimates by also providing the relative scale and shifts of depth maps.",
    "author": [
      {
        "@type": "Person",
        "name": "Yaqing Ding",
        "affiliation": {
          "@type": "Organization",
          "name": "Visal Recognition Group, CTU Prague"
        }
      },
      {
        "@type": "Person",
        "name": "Viktor Kocur",
        "affiliation": {
          "@type": "Organization",
          "name": "Comenius University Bratislava"
        }
      },
	  {
        "@type": "Person",
        "name": "Václav Vávra",
        "affiliation": {
          "@type": "Organization",
          "name": "Visal Recognition Group, CTU Prague"
        }
      },
      {
        "@type": "Person",
        "name": "Zuzana Berger Haladová",
        "affiliation": {
          "@type": "Organization",
          "name": "Comenius University Bratislava"
        }
      },
      {
        "@type": "Person",
        "name": "Jian Yang",
        "affiliation": {
          "@type": "Organization",
          "name": "VCIP, CS, Nankai University"
        }
      },
	  {
        "@type": "Person",
        "name": "Torsten Sattler",
        "affiliation": {
          "@type": "Organization",
          "name": "CIIRC, CTU Prague"
        }
      },
	        {
        "@type": "Person",
        "name": "Zuzana Kukelova",
        "affiliation": {
          "@type": "Organization",
          "name": "Visal Recognition Group, CTU Prague"
        }
      },	  
    ],
    "datePublished": "2025-10-10",
    "publisher": {
      "@type": "IEEE/CVF",
      "name": "ICCV"
    },
    "url": "https://kocurvik.github.io/reposed",
    "image": "https://kocurvik.github.io/reposed/static/images/social_preview.png",
    "keywords": ["3D vision", "relative pose", "monocular depth estimation", "machine learning", "computer vision"],
    "abstract": "Recent advances in monocular depth estimation methods (MDE) and their improved accuracy open new possibilities for their applications. In this paper, we investigate how monocular depth estimates can be used for relative pose estimation. In particular, we are interested in answering the question whether using MDEs improves results over traditional point-based methods. We propose a novel framework for estimating the relative pose of two cameras from point correspondences with associated monocular depths. Since depth predictions are typically defined up to an unknown scale or even both unknown scale and shift parameters, our solvers jointly estimate the scale or both the scale and shift parameters along with the relative pose. We derive efficient solvers considering different types of depths for three camera configurations: (1) two calibrated cameras, (2) two cameras with an unknown shared focal length, and (3) two cameras with unknown different focal lengths. Our new solvers outperform state-of-the-art depth-aware solvers in terms of speed and accuracy. In extensive real experiments on multiple datasets and with various MDEs, we discuss which depth-aware solvers are preferable in which situation.",
    "citation": "@inproceedings{ding2025reposed,
  title={RePoseD: Efficient Relative Pose Estimation With Known Depth Information},
  author={Ding, Yaqing and Kocur, Viktor and V{\'a}vra, V{\'a}clav and Haladov{\'a}, Zuzana Berger and Yang, Jian and Sattler, Torsten and Kukelova, Zuzana},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  year={2025}
}",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://kocurvik.github.io/reposed"
    },
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <!--
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "INSTITUTION_OR_LAB_NAME",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "logo": "https://YOUR_DOMAIN.com/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
  -->
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <!-- More Works Dropdown -->
  <!--
  <div class="more-works-container">
    <button class="more-works-btn" onclick="toggleMoreWorks()" title="View More Works from Our Lab">
      <i class="fas fa-flask"></i>
      More Works
      <i class="fas fa-chevron-down dropdown-arrow"></i>
    </button>
    <div class="more-works-dropdown" id="moreWorksDropdown">
      <div class="dropdown-header">
        <h4>More Works from Our Lab</h4>
        <button class="close-btn" onclick="toggleMoreWorks()">
          <i class="fas fa-times"></i>
        </button>
      </div>
      <div class="works-list">
        <a href="https://arxiv.org/abs/PAPER_ID_1" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Paper Title 1</h5>
            <p>Brief description of the work and its main contribution.</p>
            <span class="work-venue">Conference/Journal 2024</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="https://arxiv.org/abs/PAPER_ID_2" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Paper Title 2</h5>
            <p>Brief description of the work and its main contribution.</p>
            <span class="work-venue">Conference/Journal 2023</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="https://arxiv.org/abs/PAPER_ID_3" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Paper Title 3</h5>
            <p>Brief description of the work and its main contribution.</p>
            <span class="work-venue">Conference/Journal 2023</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
      </div>
    </div>
  </div>
  -->

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">RePoseD: Efficient Relative Pose Estimation With Known Depth Information</h1>
			<h2 class="title is-3 publication-authors"> ICCV 2025 Oral </h2>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                Yaqing Ding<sup>1</sup></span>
                <span class="author-block">
                  Viktor Kocur<sup>2</sup></span>
                  <span class="author-block">Václav Vávra<sup>1</sup></span>
                  <span class="author-block">Zuzana Berger Haladová<sup>2</sup></span>
                  <span class="author-block">Jian Yang<sup>3</sup></span>
                  <span class="author-block">Torsten Sattler<sup>4</sup></span>
                  <span class="author-block">Zuzana Kukelova<sup>1</sup></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Visual Recognition Group, CTU Prague,  
					<sup>2</sup>Comenius University Bratislava,<br>  
					<sup>3</sup> VCIP, CS, Nankai University,   
					<sup>4</sup>CIIRC, CTU Prague<br> 
					</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2501.07742.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <span class="link-block">
                      <a href="static/pdfs/reposed_sm.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <span class="link-block">
                    <a href="https://github.com/kocurvik/mdrp" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
				
				<span class="link-block">
                    <a href="https://github.com/kocurvik/mdrp" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-code"></i>
                    </span>
                    <span>Demo</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://arxiv.org/abs/2501.07742" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!-- TODO: Replace with your teaser video
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      	  
      <video poster="" id="tree" autoplay controls muted loop height="100%" preload="metadata">
        
        <source src="static/videos/banner_video.mp4" type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        RePoseD: Efficient Relative Pose Estimation With Known Depth Information
      </h2>
    </div>
  </div>
</section>
-->
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent advances in monocular depth estimation methods (MDE) and their improved accuracy open new possibilities for their applications. In this paper, we investigate how monocular depth estimates can be used for relative pose estimation. In particular, we are interested in answering the question whether using MDEs improves results over traditional point-based methods. We propose a novel framework for estimating the relative pose of two cameras from point correspondences with associated monocular depths. Since depth predictions are typically defined up to an unknown scale or even both unknown scale and shift parameters, our solvers jointly estimate the scale or both the scale and shift parameters along with the relative pose. We derive efficient solvers considering different types of depths for three camera configurations: (1) two calibrated cameras, (2) two cameras with an unknown shared focal length, and (3) two cameras with unknown different focal lengths. Our new solvers outperform state-of-the-art depth-aware solvers in terms of speed and accuracy. In extensive real experiments on multiple datasets and with various MDEs, we discuss which depth-aware solvers are preferable in which situation.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel 
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <img src="static/images/carousel1.jpg" alt="First research result visualization" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
          First image description.
        </h2>
      </div>
      <div class="item">
        <img src="static/images/carousel2.jpg" alt="Second research result visualization" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
          Second image description.
        </h2>
      </div>
      <div class="item">
        <img src="static/images/carousel3.jpg" alt="Third research result visualization" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
         Third image description.
       </h2>
     </div>
     <div class="item">
      <img src="static/images/carousel4.jpg" alt="Fourth research result visualization" loading="lazy"/>
      <h2 class="subtitle has-text-centered">
        Fourth image description.
      </h2>
    </div>
  </div>
</div>
</div>
</section>
-->




<!-- Youtube video
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
-->


<!-- Video carousel
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" controls muted loop height="100%" preload="metadata">
            
            <source src="static/videos/carousel1.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          
          <video poster="" id="video2" controls muted loop height="100%" preload="metadata">
            
            <source src="static/videos/carousel2.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          
          <video poster="" id="video3" controls muted loop height="100%" preload="metadata">
          
            <source src="static/videos/carousel3.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
-->



<section class="hero is-small is-light">
  <div class="hero-body">
  <div class="container">
  
  
<div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
  <div class="content has-text-justified ">
  <h3 class="title is-8 has-text-centered">Extended Results</h3>
  <p>
  Since the camera-ready version of the paper we have implemented an improved optimization strategy. Similarly to MADPose, we optimize the Sampson error jointly with both the forward and backward reprojection errors. The implementation of this strategy within PoseLib leads to significantly better resutls compared to those reported in our paper. Our method with the improved strategy surpasses the results of MADPose while being 10-20x faster. Below we provide a sample of these results on the Phototourism dataset for the calibrated case.
  </p>
  </div>
  </div>
  </div>
  
                    <div class="column has-text-centered">
                    <div class="publication-links">
                      <span class="link-block">
                        <a href="https://github.com/kocurvik/mdrp/blob/main/EXTENDED_RESULTS.md" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-table"></i>
                        </span>
                        <span>Full Extended Results</span>
                      </a>
                    </span>
					</div>
					</div>
  
  
  
  <h3 class="title is-8 has-text-centered">Preview - PhotoTourism (calibrated)</h3>
  
  <table class="table is-striped is-hoverable centered-table">
<tr><td rowspan="2"  style="vertical-align : middle;text-align:center;">Depth</td><td rowspan="2"  style="vertical-align : middle;text-align:center;">Method</td><td rowspan="2"  style="vertical-align : middle;text-align:center;">Scale</td><td rowspan="2"  style="vertical-align : middle;text-align:center;">Shift</td><td colspan="3" align="center">SP+LG</td><td align="center" colspan="3">RoMA</td></tr>
<tr><td>$\epsilon(^\circ)\downarrow$</td><td>mAA $\uparrow$</td><td>Runtime (ms)</td><td>$\epsilon(^\circ)\downarrow$</td><td>mAA $\uparrow$</td><td>Runtime (ms)</td></tr>
<td rowspan="1" style="vertical-align : middle;text-align:center;">-</td>
<td>5-Point</td><td></td><td></td> <td>1.42</td><td>76.56</td><td>63.79</td><td>0.78</td><td>86.18</td><td>264.61</td>
</tr>
<td rowspan="5" style="vertical-align : middle;text-align:center;">MoGe</td>
<td>3P-RelDepth</td><td></td><td></td> <td>8.12</td><td>53.40</td><td>55.85</td><td>1.69</td><td>67.22</td><td>221.06</td>
</tr>
<tr>
<td>P3P</td><td></td><td></td> <td>1.40</td><td>77.37</td><td>32.95</td><td>0.78</td><td>86.42</td><td>148.76</td>
</tr>
<tr>
<td>MADPose</td><td>✔</td><td>✔</td> <td>1.27</td><td>80.28</td><td>788.18</td><td>0.87</td><td>86.85</td><td>1753.49</td>
</tr>
<tr>
<td>Ours</td><td>✔</td><td>✔</td> <td><strong>1.24</strong></td><td><strong>81.34</strong></td><td><strong>28.93</strong></td><td><strong>0.74</strong></td><td><strong>88.58</strong></td><td><strong>125.66</strong></td>
</tr>
<tr>
<td>Ours</td><td>✔</td><td></td> <td>1.75</td><td>80.29</td><td>30.11</td><td>1.03</td><td>88.02</td><td>135.95</td>
</tr>
<td rowspan="5" style="vertical-align : middle;text-align:center;">UniDepth</td>
<td>3P-RelDepth</td><td></td><td></td> <td>4.07</td><td>51.60</td><td>52.49</td><td>1.33</td><td>67.56</td><td>214.73</td>
</tr>
<tr>
<td>P3P</td><td></td><td></td> <td>1.40</td><td>77.47</td><td>34.30</td><td>0.78</td><td>86.43</td><td>150.95</td>
</tr>
<tr>
<td>MADPose</td><td>✔</td><td>✔</td> <td>1.15</td><td>82.09</td><td>720.34</td><td>0.78</td><td>87.60</td><td>1695.57</td>
</tr>
<tr>
<td>Ours</td><td>✔</td><td>✔</td> <td><strong>1.04</strong></td><td>83.71</td><td><strong>30.88</strong></td><td><strong>0.69</strong></td><td>89.27</td><td><strong>131.52</strong></td>
</tr>
<tr>
<td>Ours</td><td>✔</td><td></td> <td>1.16</td><td><strong>84.56</strong></td><td>31.19</td><td>0.81</td><td><strong>90.18</strong></td><td>137.26</td>
</tr>
</table>


<table class="table is-striped is-hoverable centered-table">
<tr><td rowspan="2"  style="vertical-align : middle;text-align:center;">Depth</td><td rowspan="2"  style="vertical-align : middle;text-align:center;">Method</td><td rowspan="2"  style="vertical-align : middle;text-align:center;">Scale</td><td rowspan="2"  style="vertical-align : middle;text-align:center;">Shift</td><td colspan="3" align="center">MASt3R</td>
<tr><td>$\epsilon(^\circ)\downarrow$</td><td>mAA $\uparrow$</td><td>Runtime (ms)</td></tr>
<td rowspan="1" style="vertical-align : middle;text-align:center;">-</td>
<td>5-Point</td><td></td><td></td> <td>1.14</td><td>81.66</td><td>137.75</td>
</tr>
<tr>
<td rowspan="5" style="vertical-align : middle;text-align:center;">MASt3R</td>
<td>3P-RelDepth</td><td></td><td></td> <td><strong>1.13</strong></td><td>80.83</td><td>149.86</td>
</tr>
<tr>
<td>P3P</td><td></td><td></td> <td><strong>1.13</strong></td><td><strong>81.50</strong></td><td><strong>66.06</strong></td>
</tr>
<tr>
<td>MADPose</td><td>✔</td><td>✔</td> <td>2.10</td><td>72.14</td><td>2154.89</td>
</tr>
<tr>
<td>Ours</td><td>✔</td><td>✔</td> <td>29.59</td><td>1.27</td><td>95.77</td>
</tr>
<tr>
<td>Ours</td><td>✔</td><td></td> <td>29.36</td><td>1.28</td><td>121.02</td>
</tr>
</table>
  </div>
  </div>
  </section>



<!-- Paper poster
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      
      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
-->



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@inproceedings{ding2025reposed,
  title={RePoseD: Efficient Relative Pose Estimation With Known Depth Information},
  author={Ding, Yaqing and Kocur, Viktor and V{\'a}vra, V{\'a}clav and Haladov{\'a}, Zuzana Berger and Yang, Jian and Sattler, Torsten and Kukelova, Zuzana},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  year={2025}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-14YB01LDD6"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-14YB01LDD6');
</script>
    <!-- End of Statcounter Code -->

  </body>
  </html>
